{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLDR:\n",
    "- Hyperparameters affect the convergence of the training:\n",
    "  - Learning rate is the amount the optimizer affects the parameters at each optimization step\n",
    "  - Batch size is the number of samples processed before updating parameters\n",
    "  - Epochs is the number of times the training loops over the entire dataset\n",
    "- The training loop calculates for each batch the loss and optimizes the model:\n",
    "  - A loss function defines how the loss is determined\n",
    "  - The optimizer provides logic for how the model is optimized as a result\n",
    "- The testing loop evaluates the model after each epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Refer to \"2. Datasets & DataLoaders\"\n",
    "training_data = datasets.FashionMNIST(\n",
    "  root=\"Fashion-MNIST\",\n",
    "  train=True,\n",
    "  download=True,\n",
    "  transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "  root=\"Fashion-MNIST\",\n",
    "  train=False,\n",
    "  download=True,\n",
    "  transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# Refer to \"4. Build the Neural Network\"\n",
    "class MyNeuralNetwork(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.linear_relu_stack = nn.Sequential(\n",
    "      nn.Linear(28 * 28, 512),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(512, 512),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(512, 10),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.flatten(x)\n",
    "    logits = self.linear_relu_stack(x)\n",
    "    return logits\n",
    "\n",
    "model = MyNeuralNetwork()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the steps in the previous notebooks, we can initialize the model by:\n",
    "- Getting the datasets\n",
    "- Creating DataLoaders from the datasets\n",
    "- Defining the structure of the neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters, as shown below, impact the model\n",
    "# training and convergent rates. Note that there *are*\n",
    "# optimal hyperparameters, but when prototyping, they\n",
    "# don't matter so much\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters affect the convergence of the model:\n",
    "- Learning rate corresponds to slower learning speed and more unpredictable training behavior at lower and higher values respectively\n",
    "- Batch size is the number of data samples propogated through the network before the parameters are updated\n",
    "- Number of epochs is the number of times the training loops over the entire dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Training and Testing Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch #1 ***\n",
      "Training...\n",
      "Loss: 2.29855251, [  0/938]\n",
      "Loss: 2.29045606, [100/938]\n",
      "Loss: 2.27337646, [200/938]\n",
      "Loss: 2.26828003, [300/938]\n",
      "Loss: 2.24884081, [400/938]\n",
      "Loss: 2.21866322, [500/938]\n",
      "Loss: 2.21745205, [600/938]\n",
      "Loss: 2.18242002, [700/938]\n",
      "Loss: 2.17854881, [800/938]\n",
      "Loss: 2.14892769, [900/938]\n",
      "Testing...\n",
      "Accuracy: 0.51%, average loss: 2.14140081\n",
      "\n",
      "*** Epoch #2 ***\n",
      "Training...\n",
      "Loss: 2.14595199, [  0/938]\n",
      "Loss: 2.14124417, [100/938]\n",
      "Loss: 2.07956338, [200/938]\n",
      "Loss: 2.10073686, [300/938]\n",
      "Loss: 2.04285312, [400/938]\n",
      "Loss: 1.98154294, [500/938]\n",
      "Loss: 2.00525117, [600/938]\n",
      "Loss: 1.92100573, [700/938]\n",
      "Loss: 1.93031168, [800/938]\n",
      "Loss: 1.85450912, [900/938]\n",
      "Testing...\n",
      "Accuracy: 0.61%, average loss: 1.84844100\n",
      "\n",
      "*** Epoch #3 ***\n",
      "Training...\n",
      "Loss: 1.88395309, [  0/938]\n",
      "Loss: 1.85617280, [100/938]\n",
      "Loss: 1.73068535, [200/938]\n",
      "Loss: 1.77646601, [300/938]\n",
      "Loss: 1.66236997, [400/938]\n",
      "Loss: 1.62110662, [500/938]\n",
      "Loss: 1.64127755, [600/938]\n",
      "Loss: 1.54362488, [700/938]\n",
      "Loss: 1.57724297, [800/938]\n",
      "Loss: 1.46959651, [900/938]\n",
      "Testing...\n",
      "Accuracy: 0.63%, average loss: 1.47932851\n",
      "\n",
      "*** Epoch #4 ***\n",
      "Training...\n",
      "Loss: 1.55141103, [  0/938]\n",
      "Loss: 1.52242815, [100/938]\n",
      "Loss: 1.36939216, [200/938]\n",
      "Loss: 1.44307160, [300/938]\n",
      "Loss: 1.32716167, [400/938]\n",
      "Loss: 1.32573414, [500/938]\n",
      "Loss: 1.33912253, [600/938]\n",
      "Loss: 1.26485813, [700/938]\n",
      "Loss: 1.30627561, [800/938]\n",
      "Loss: 1.20680749, [900/938]\n",
      "Testing...\n",
      "Accuracy: 0.65%, average loss: 1.22219396\n",
      "\n",
      "*** Epoch #5 ***\n",
      "Training...\n",
      "Loss: 1.30260682, [  0/938]\n",
      "Loss: 1.29473734, [100/938]\n",
      "Loss: 1.12500226, [200/938]\n",
      "Loss: 1.23188257, [300/938]\n",
      "Loss: 1.11115110, [400/938]\n",
      "Loss: 1.13217676, [500/938]\n",
      "Loss: 1.15709460, [600/938]\n",
      "Loss: 1.09147394, [700/938]\n",
      "Loss: 1.13725746, [800/938]\n",
      "Loss: 1.05324781, [900/938]\n",
      "Testing...\n",
      "Accuracy: 0.66%, average loss: 1.06260920\n",
      "\n",
      "*** Epoch #6 ***\n",
      "Training...\n",
      "Loss: 1.13633823, [  0/938]\n",
      "Loss: 1.15194583, [100/938]\n",
      "Loss: 0.96316653, [200/938]\n",
      "Loss: 1.10039520, [300/938]\n",
      "Loss: 0.97901255, [400/938]\n",
      "Loss: 1.00282526, [500/938]\n",
      "Loss: 1.04611671, [600/938]\n",
      "Loss: 0.98189235, [700/938]\n",
      "Loss: 1.02797580, [800/938]\n",
      "Loss: 0.95691931, [900/938]\n",
      "Testing...\n",
      "Accuracy: 0.67%, average loss: 0.95958292\n",
      "\n",
      "*** Epoch #7 ***\n",
      "Training...\n",
      "Loss: 1.02108192, [  0/938]\n",
      "Loss: 1.05997562, [100/938]\n",
      "Loss: 0.85258812, [200/938]\n",
      "Loss: 1.01349378, [300/938]\n",
      "Loss: 0.89622468, [400/938]\n",
      "Loss: 0.91271055, [500/938]\n",
      "Loss: 0.97406024, [600/938]\n",
      "Loss: 0.91062272, [700/938]\n",
      "Loss: 0.95320863, [800/938]\n",
      "Loss: 0.89196742, [900/938]\n",
      "Testing...\n",
      "Accuracy: 0.69%, average loss: 0.88916624\n",
      "\n",
      "*** Epoch #8 ***\n",
      "Training...\n",
      "Loss: 0.93688047, [  0/938]\n",
      "Loss: 0.99550164, [100/938]\n",
      "Loss: 0.77326393, [200/938]\n",
      "Loss: 0.95211643, [300/938]\n",
      "Loss: 0.84127676, [400/938]\n",
      "Loss: 0.84720713, [500/938]\n",
      "Loss: 0.92368585, [600/938]\n",
      "Loss: 0.86261469, [700/938]\n",
      "Loss: 0.89977920, [800/938]\n",
      "Loss: 0.84472227, [900/938]\n",
      "Testing...\n",
      "Accuracy: 0.70%, average loss: 0.83819956\n",
      "\n",
      "*** Epoch #9 ***\n",
      "Training...\n",
      "Loss: 0.87220210, [  0/938]\n",
      "Loss: 0.94639724, [100/938]\n",
      "Loss: 0.71374953, [200/938]\n",
      "Loss: 0.90627265, [300/938]\n",
      "Loss: 0.80196428, [400/938]\n",
      "Loss: 0.79819751, [500/938]\n",
      "Loss: 0.88566679, [600/938]\n",
      "Loss: 0.82885683, [700/938]\n",
      "Loss: 0.86024588, [800/938]\n",
      "Loss: 0.80824238, [900/938]\n",
      "Testing...\n",
      "Accuracy: 0.71%, average loss: 0.79931903\n",
      "\n",
      "*** Epoch #10 ***\n",
      "Training...\n",
      "Loss: 0.82053125, [  0/938]\n",
      "Loss: 0.90625054, [100/938]\n",
      "Loss: 0.66719306, [200/938]\n",
      "Loss: 0.87053710, [300/938]\n",
      "Loss: 0.77194840, [400/938]\n",
      "Loss: 0.76059765, [500/938]\n",
      "Loss: 0.85471725, [600/938]\n",
      "Loss: 0.80413049, [700/938]\n",
      "Loss: 0.82941335, [800/938]\n",
      "Loss: 0.77882642, [900/938]\n",
      "Testing...\n",
      "Accuracy: 0.72%, average loss: 0.76824719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To train the model, we must define a training loop.\n",
    "# Training is done in a few steps:\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "  batch_count = len(dataloader)\n",
    "  for batch, (x, y) in enumerate(dataloader):\n",
    "    # Calculate predictions\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Calculate loss and gradients with respect to\n",
    "    # the parameters:\n",
    "    # - Loss functions measure how far off the\n",
    "    #   predictions are from the actual data\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimize the parameters using the gradients using\n",
    "    # an optimizer:\n",
    "    # - Optimizers are optimization algorithms\n",
    "    #   containing logic for how to optimize parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log the progress of the training\n",
    "    if batch % 100 == 0:\n",
    "      print(f\"Loss: {loss:>0.8f}, [{batch:>3d}/{batch_count:>3d}]\")\n",
    "\n",
    "# After training, the model must be evaluated. For this,\n",
    "# we can define a testing loop, which uses a different\n",
    "# dataloader than the training loop to test the model.\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "  batch_count = len(dataloader)\n",
    "  pred_count = len(dataloader.dataset)\n",
    "  total_loss, total_correct = 0, 0\n",
    "\n",
    "  for (x, y) in dataloader:\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    correct = y_pred.argmax(1) == y\n",
    "    \n",
    "    total_loss += loss\n",
    "    total_correct += correct.sum()\n",
    "\n",
    "  # What metric is used when testing the model is up\n",
    "  # to the creator, but usually it's average accuracy\n",
    "  # and/or loss\n",
    "  avg_loss = total_loss / batch_count\n",
    "  avg_correct = total_correct / pred_count\n",
    "\n",
    "  print(f\"Accuracy: {avg_correct:>.2f}%, average loss: {avg_loss:>0.8f}\")\n",
    "\n",
    "# `CrossEntropyLoss()` calculates the loss when the\n",
    "# model is used to classify between more than two classes\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# `SGD()` is short for \"Stochastic Gradient Descent\"\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# The training loop and testing loop are repeatedly\n",
    "# executed for the total number of epochs\n",
    "for t in range(epochs):\n",
    "  print(f\"*** Epoch #{t + 1} ***\")\n",
    "  print(\"Training...\")\n",
    "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "  print(\"Testing...\")\n",
    "  test_loop(train_dataloader, model, loss_fn)\n",
    "  print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and testing loops define how the model is trained and evaluated, respectively. Typically, the following are required:\n",
    "- A DataLoader, containing all the data in batches\n",
    "- A loss function, to determine how far off the model's predictions are\n",
    "- An optimizer, to guide the model towards better predictions\n",
    "- The hyperparameters, which affect convergence of the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
