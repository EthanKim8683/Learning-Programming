{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLDR:\n",
    "- Parameters can be differentiated if specified\n",
    "- Expressions form graphs, which:\n",
    "  - Handle forward passes, calculating the values of each node\n",
    "  - Handle backward passes, calculating the derivatives of each node\n",
    "- Gradients can be generated using `.backward()` and are accumulated in `.grad`\n",
    "- Gradients can be disabled to freeze parameters or improve performance if the backward pass isn't needed "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Glance at Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function of `y_pred`: <AddBackward0 object at 0x00000223F9B13E80>\n",
      "Gradient function of `loss`: <BinaryCrossEntropyWithLogitsBackward0 object at 0x00000223954417B0>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Parameters can be represented using tensors, which, if\n",
    "# specified using `requires_grad=True` in the constructor\n",
    "# (or calling `.requires_grad_() after creation), can be\n",
    "# differentiated\n",
    "w = torch.randn(5, 3, requires_grad=True) # Weights\n",
    "b = torch.randn(3)                        # Bias\n",
    "y = torch.zeros(3)                        # y-actual\n",
    "x = torch.ones(5)\n",
    "b.requires_grad_()\n",
    "\n",
    "# Furthermore, when put inside an expression, the parameters\n",
    "# and operators are pieced together into a \"computational\n",
    "# graph\":\n",
    "# - The function used to construct this graph is an object\n",
    "#   called `Function`, which stores:\n",
    "#   - How to compute the value of the function in the\n",
    "#     *forward* direction (`.forward()`; do not call\n",
    "#     directly!)\n",
    "#   - How to compute the derivative of the function in the\n",
    "#     *backward* propogation step (`.backward()`)\n",
    "y_pred = torch.matmul(x, w) + b           # y-predicted\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(y_pred, y)\n",
    "print(f\"Gradient function of `y_pred`: {y_pred.grad_fn}\")\n",
    "print(f\"Gradient function of `loss`: {loss.grad_fn}\")\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters can be linked together and form a computational graph, which handles:\n",
    "- Evaluating the expression in the forward direction: computing the values of each node\n",
    "- Evaluating the expression in the backward direction: computing the derivatives of each node"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated gradient of `w` after 1 call:\n",
      "tensor([[0.2526, 0.1327, 0.0793],\n",
      "        [0.2526, 0.1327, 0.0793],\n",
      "        [0.2526, 0.1327, 0.0793],\n",
      "        [0.2526, 0.1327, 0.0793],\n",
      "        [0.2526, 0.1327, 0.0793]])\n",
      "Accumulated gradient of `b` after 1 call:\n",
      "tensor([0.2526, 0.1327, 0.0793])\n",
      "\n",
      "Accumulated gradient of `w` after 2 calls:\n",
      "tensor([[0.5052, 0.2654, 0.1586],\n",
      "        [0.5052, 0.2654, 0.1586],\n",
      "        [0.5052, 0.2654, 0.1586],\n",
      "        [0.5052, 0.2654, 0.1586],\n",
      "        [0.5052, 0.2654, 0.1586]])\n",
      "Accumulated gradient of `b` after 2 calls:\n",
      "tensor([0.5052, 0.2654, 0.1586])\n",
      "\n",
      "Accumulated gradient of `w` after zeroing:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Accumulated gradient of `b` after zeroing:\n",
      "tensor([0., 0., 0.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To compute and use derivatives:\n",
    "# - First, `.backward()` can be called on the root, which\n",
    "#   backward propogates to all nodes in its graph, computing\n",
    "#   the gradients for each one along the way\n",
    "# - Then, the gradients are accumulated in the `.grad`\n",
    "#   property of each descendant of the root\n",
    "#   - If `.backward()` is called multiple times while\n",
    "#     retaining graph (`retain_graph=True` in `.backward()`),\n",
    "#     the `.grad` property will contain the sum of the\n",
    "#     gradients\n",
    "#   - Gradients can be set to zero by calling `.grad.zero_()`\n",
    "loss.backward(retain_graph=True)\n",
    "print(f\"Accumulated gradient of `w` after 1 call:\\n{w.grad}\")\n",
    "print(f\"Accumulated gradient of `b` after 1 call:\\n{b.grad}\")\n",
    "print()\n",
    "\n",
    "loss.backward()\n",
    "print(f\"Accumulated gradient of `w` after 2 calls:\\n{w.grad}\")\n",
    "print(f\"Accumulated gradient of `b` after 2 calls:\\n{b.grad}\")\n",
    "print()\n",
    "\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(f\"Accumulated gradient of `w` after zeroing:\\n{w.grad}\")\n",
    "print(f\"Accumulated gradient of `b` after zeroing:\\n{b.grad}\")\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `.backward()` on a root will differentiate all the parameters contributing to it and add the resulting gradients to each node's `.grad` property"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disabling Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`.requires_grad` before disabling gradients: True\n",
      "\n",
      "`.requires_grad` after disabling gradients with `.no_grad()`: False\n",
      "\n",
      "`.requires_grad` after disabling gradients with `.detach()`: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradients can be permanently or temporarily disabled. This\n",
    "# could be done to:\n",
    "# - Mark some parameters as \"frozen parameters\"\n",
    "# - Speed up computations when only the forward pass is needed,\n",
    "#   since extra work is required to track these gradients\n",
    "y_pred = torch.matmul(x, w) + b\n",
    "print(f\"`.requires_grad` before disabling gradients: {y_pred.requires_grad}\")\n",
    "print()\n",
    "\n",
    "# Disabling gradients with `.no_grad()`\n",
    "with torch.no_grad():\n",
    "  y_pred = torch.matmul(x, w) + b\n",
    "print(f\"`.requires_grad` after disabling gradients with `.no_grad()`: {y_pred.requires_grad}\")\n",
    "print()\n",
    "\n",
    "# Disabling gradients by getting a new tensor, detached from\n",
    "# the graph, using `.detach()`\n",
    "y_pred = torch.matmul(x, w) + b\n",
    "y_pred = y_pred.detach()\n",
    "print(f\"`.requires_grad` after disabling gradients with `.detach()`: {y_pred.requires_grad}\")\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients can be disabled to freeze parameters or improve performance if only the forward pass is needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "There is an optional reading section on tensor gradients and Jacobian products\n",
    "\n",
    "Since I don't understand it yet, I'll leave it to my future self to take notes on it in my stead"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
