{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLDR:\n",
    "- Custom neural networks are defined as a subclass of `Module()`\n",
    "- Components can be combined sequentially to form a network\n",
    "- Models and their parameters can be inspected using various methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Training Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Use the best possible available device for training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking `.is_available()` on various Pytorch backends can be used to find the optimal device for training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Custom Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 9\n"
     ]
    }
   ],
   "source": [
    "# We can define a neural network as a subclass of\n",
    "# `Module()`\n",
    "class MyNeuralNetwork(nn.Module):\n",
    "  # `__init__` initializes the structure of the network\n",
    "  # as well as the parameters and names of its components\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.linear_relu_stack = nn.Sequential(\n",
    "      nn.Linear(28 * 28, 512),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(512, 512),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(512, 10),\n",
    "    )\n",
    "\n",
    "  # `forward` implements the operations done on the input\n",
    "  # data, or, in simpler terms, what the model does\n",
    "  def forward(self, x):\n",
    "    x = self.flatten(x)\n",
    "    logits = self.linear_relu_stack(x)\n",
    "    return logits\n",
    "\n",
    "# Initialize necessary modules\n",
    "model = MyNeuralNetwork().to(device)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "# Show scaled model output\n",
    "x = torch.rand((1, 28, 28)).to(device)\n",
    "probs = softmax(model(x))\n",
    "y_pred = probs.argmax(1)\n",
    "print(f\"Predicted class: {y_pred.item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining a custom neural network, the following needs to be defined:\n",
    "- The structure of the model\n",
    "- What the model does"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images size: torch.Size([3, 28, 28])\n",
      "\n",
      "Size after flatten layer: torch.Size([3, 784])\n",
      "\n",
      "Size after linear layer: torch.Size([3, 20])\n",
      "\n",
      "Before ReLU: tensor([[-0.1457,  0.3225,  0.1064, -0.3286, -0.0615, -0.0990, -0.1113, -0.1257,\n",
      "         -0.1581,  0.2575, -0.0024, -0.1774, -0.0598,  0.0683,  0.3196,  0.2255,\n",
      "         -0.2396, -0.4253,  0.3154, -0.2214],\n",
      "        [-0.3872,  0.4018, -0.0428, -0.2538, -0.2778,  0.1549,  0.0825, -0.2577,\n",
      "         -0.1063,  0.6018, -0.1120, -0.1484, -0.0986, -0.0574,  0.4554,  0.2962,\n",
      "         -0.0538,  0.2258,  0.3814, -0.2245],\n",
      "        [ 0.0629,  0.7890, -0.0446, -0.2031, -0.0312,  0.3248, -0.0231, -0.1580,\n",
      "         -0.0016, -0.2692,  0.3072, -0.0067,  0.0764, -0.1213,  0.3454, -0.1110,\n",
      "          0.0645, -0.1058,  0.3771, -0.4963]], grad_fn=<AddmmBackward0>)\n",
      "After ReLU: tensor([[0.0000, 0.3225, 0.1064, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2575, 0.0000, 0.0000, 0.0000, 0.0683, 0.3196, 0.2255, 0.0000, 0.0000,\n",
      "         0.3154, 0.0000],\n",
      "        [0.0000, 0.4018, 0.0000, 0.0000, 0.0000, 0.1549, 0.0825, 0.0000, 0.0000,\n",
      "         0.6018, 0.0000, 0.0000, 0.0000, 0.0000, 0.4554, 0.2962, 0.0000, 0.2258,\n",
      "         0.3814, 0.0000],\n",
      "        [0.0629, 0.7890, 0.0000, 0.0000, 0.0000, 0.3248, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.3072, 0.0000, 0.0764, 0.0000, 0.3454, 0.0000, 0.0645, 0.0000,\n",
      "         0.3771, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "Logits: tensor([[0.0000, 0.5348, 0.0788, 0.0000, 0.0000, 0.2688, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2092, 0.0000, 0.0000, 0.0000,\n",
      "         0.4804, 0.0000],\n",
      "        [0.0000, 0.4728, 0.0041, 0.0000, 0.0000, 0.0699, 0.0000, 0.0221, 0.0145,\n",
      "         0.0199, 0.2140, 0.0000, 0.0177, 0.0000, 0.1223, 0.0000, 0.3146, 0.0000,\n",
      "         0.1268, 0.0000],\n",
      "        [0.0000, 0.1280, 0.1993, 0.0000, 0.0000, 0.3735, 0.0342, 0.0000, 0.0238,\n",
      "         0.0957, 0.1276, 0.1591, 0.0000, 0.0696, 0.2277, 0.0000, 0.2123, 0.0000,\n",
      "         0.4441, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "Softmax of logits: tensor([[0.0000, 0.5348, 0.0788, 0.0000, 0.0000, 0.2688, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2092, 0.0000, 0.0000, 0.0000,\n",
      "         0.4804, 0.0000],\n",
      "        [0.0000, 0.4728, 0.0041, 0.0000, 0.0000, 0.0699, 0.0000, 0.0221, 0.0145,\n",
      "         0.0199, 0.2140, 0.0000, 0.0177, 0.0000, 0.1223, 0.0000, 0.3146, 0.0000,\n",
      "         0.1268, 0.0000],\n",
      "        [0.0000, 0.1280, 0.1993, 0.0000, 0.0000, 0.3735, 0.0342, 0.0000, 0.0238,\n",
      "         0.0957, 0.1276, 0.1591, 0.0000, 0.0696, 0.2277, 0.0000, 0.2123, 0.0000,\n",
      "         0.4441, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a minibatch of three 28x28 images\n",
    "images = torch.rand(3, 28, 28).to(device)\n",
    "print(f\"Images size: {images.size()}\")\n",
    "print()\n",
    "\n",
    "# The `Flatten()` module is in charge of flattening each image\n",
    "# in the minibatch into a contiguous array. The default\n",
    "# arguments of `Flatten()` preserve the minibatch dimension\n",
    "flatten = nn.Flatten()\n",
    "after_flatten = flatten(images)\n",
    "print(f\"Size after flatten layer: {after_flatten.size()}\")\n",
    "print()\n",
    "\n",
    "# The `Linear()` module passes each of its input features\n",
    "# through a linear transformation `m * x + b`, where `m`,\n",
    "# `x` and `b` are the weights, input features and biases\n",
    "# respectively\n",
    "linear = nn.Linear(in_features=28 * 28, out_features=20)\n",
    "after_linear = linear(after_flatten)\n",
    "print(f\"Size after linear layer: {after_linear.size()}\")\n",
    "print()\n",
    "\n",
    "# The `ReLU()` module applies a non-linearity to the input\n",
    "# features, which provides the model a means of approximating\n",
    "# the shape of the true mapping between the input and outputs\n",
    "# of the model\n",
    "relu = nn.ReLU()\n",
    "after_relu = relu(after_linear)\n",
    "print(f\"Before ReLU: {after_linear}\")\n",
    "print(f\"After ReLU: {after_relu}\")\n",
    "print()\n",
    "\n",
    "# `Sequential` is an ordered container of modules, passing\n",
    "# data through all its modules in the order in which they are\n",
    "# defined\n",
    "sequential = nn.Sequential(\n",
    "  flatten,\n",
    "  linear,\n",
    "  relu,\n",
    ")\n",
    "images = torch.rand(3, 28, 28).to(device)\n",
    "logits = sequential(images)\n",
    "print(f\"Logits: {logits}\")\n",
    "print()\n",
    "\n",
    "# Since the final layer of the neural network returns\n",
    "# \"logits\", values in range `(-infinity, infinity)`, the\n",
    "# `Softmax()` module can be used to scale the logits to\n",
    "# predicted probabilities for each class\n",
    "softmax = nn.Softmax(dim=1)\n",
    "probs = softmax(logits)\n",
    "print(f\"Softmax of logits: {logits}\")\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use different layers to represent different operations, and piece them together sequentially"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper Look at the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: MyNeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Name: linear_relu_stack.0.weight, size: torch.Size([512, 784]), values:\n",
      "tensor([[-0.0083, -0.0301, -0.0188,  ...,  0.0101,  0.0274,  0.0128],\n",
      "        [ 0.0261,  0.0138, -0.0259,  ...,  0.0073, -0.0028,  0.0069]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Name: linear_relu_stack.0.bias, size: torch.Size([512]), values:\n",
      "tensor([-0.0233, -0.0044], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Name: linear_relu_stack.2.weight, size: torch.Size([512, 512]), values:\n",
      "tensor([[ 0.0367,  0.0334,  0.0374,  ..., -0.0055,  0.0107, -0.0053],\n",
      "        [-0.0197,  0.0325,  0.0245,  ...,  0.0338,  0.0339,  0.0318]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Name: linear_relu_stack.2.bias, size: torch.Size([512]), values:\n",
      "tensor([ 0.0219, -0.0362], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Name: linear_relu_stack.4.weight, size: torch.Size([10, 512]), values:\n",
      "tensor([[-0.0132, -0.0164,  0.0184,  ...,  0.0302, -0.0439, -0.0025],\n",
      "        [-0.0235,  0.0239,  0.0234,  ..., -0.0011, -0.0064,  0.0076]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Name: linear_relu_stack.4.bias, size: torch.Size([10]), values:\n",
      "tensor([0.0124, 0.0331], grad_fn=<SliceBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print model structure\n",
    "print(f\"Model structure: {model}\")\n",
    "print()\n",
    "\n",
    "# Print each named parameter\n",
    "for name, param in model.named_parameters():\n",
    "  print(f\"Name: {name}, size: {param.size()}, values:\\n{param[:2]}\")\n",
    "  print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various ways of representing what is inside the model:\n",
    "- Stringifying the model results in a string showing the structure and components of the model\n",
    "- `.named_parameters()` returns the names and values of all named parameters within the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
