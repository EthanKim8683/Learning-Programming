{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "class GomokuEnvironment:\n",
    "  BLACK, WHITE, EMPTY = 1, -1, 0\n",
    "\n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "\n",
    "    self.reset()\n",
    "\n",
    "    self.w = torch.zeros((729), dtype=torch.float32).to(device)\n",
    "  \n",
    "  def __str__(self):\n",
    "    rv = \"\"\n",
    "    for i in range(self.n):\n",
    "      for j in range(self.n):\n",
    "        if self.state[i, j] == GomokuEnvironment.BLACK:\n",
    "          rv += \"X \"\n",
    "        if self.state[i, j] == GomokuEnvironment.WHITE:\n",
    "          rv += \"O \"\n",
    "        if self.state[i, j] == GomokuEnvironment.EMPTY:\n",
    "          rv += \"- \"\n",
    "      rv = rv[:-1] + '\\n'\n",
    "    return rv[:-1]\n",
    "\n",
    "  def __init_action_space(self):\n",
    "    self.action_space = set()\n",
    "    for i in range(self.n):\n",
    "      for j in range(self.n):\n",
    "        self.action_space.add((i, j))\n",
    "  \n",
    "  def __init_contrib_total(self):\n",
    "    self.contrib_total = torch.zeros((729), dtype=torch.float32).to(device)\n",
    "    for i in range(self.n):\n",
    "      for j in range(self.n):\n",
    "        if i + 6 <= self.n:\n",
    "          self.contrib_total[364] += 1\n",
    "        if j + 6 <= self.n:\n",
    "          self.contrib_total[364] += 1\n",
    "        if i + 6 and j + 6 <= self.n:\n",
    "          self.contrib_total[364] += 2\n",
    "\n",
    "  def __update_contrib_total(self, i, j, di, dj, s):\n",
    "    i_0, j_0, k_0 = i, j, 0\n",
    "    while self.is_inside(i_0 - di, j_0 - dj) and k_0 - 1 >= -5:\n",
    "      i_0 -= di\n",
    "      j_0 -= dj\n",
    "      k_0 -= 1\n",
    "\n",
    "    i_f, j_f, k_f, mask_0, mask_f = i_0, j_0, k_0, 0, 0 \n",
    "    while self.is_inside(i_f, j_f) and k_f <= 5:\n",
    "      cell_0 = self.state[i_f, j_f].item()\n",
    "      cell_f = self.player if i_f == i and j_f == j else cell_0\n",
    "\n",
    "      mask_0 = (mask_0 * 3 + cell_0 + 1) % 729\n",
    "      mask_f = (mask_f * 3 + cell_f + 1) % 729\n",
    "\n",
    "      if k_f - k_0 >= 5:\n",
    "        self.contrib_total[mask_0] -= s\n",
    "        self.contrib_total[mask_f] += s\n",
    "\n",
    "      i_f += di\n",
    "      j_f += dj\n",
    "      k_f += 1\n",
    "  \n",
    "  def is_inside(self, i, j):\n",
    "    return i >= 0 and i < self.n and j >= 0 and j < self.n\n",
    "\n",
    "  def get_utility(self):\n",
    "    return self.utility\n",
    "\n",
    "  def get_eval(self):\n",
    "    return torch.dot(self.w, self.contrib_total)\n",
    "  \n",
    "  def get_eval_grad(self):\n",
    "    return self.contrib_total.clone()\n",
    "\n",
    "  def get_action_space(self):\n",
    "    return self.action_space\n",
    "  \n",
    "  def get_player(self):\n",
    "    return self.player\n",
    "  \n",
    "  def is_begin(self):\n",
    "    return len(self.path) == 0\n",
    "  \n",
    "  def is_end(self):\n",
    "    return self.utility != 0 or len(self.action_space) == 0\n",
    "      \n",
    "  def forward(self, a):\n",
    "    self.path.append(a)\n",
    "    i, j = a\n",
    "    \n",
    "    self.action_space.remove(a)\n",
    "\n",
    "    self.__update_contrib_total(i, j, 0, 1, 1)\n",
    "    self.__update_contrib_total(i, j, 1, 0, 1)\n",
    "    self.__update_contrib_total(i, j, 1, 1, 1)\n",
    "    self.__update_contrib_total(i, j, 1, -1, 1)\n",
    "    self.state[i, j] = self.player\n",
    "\n",
    "    if self.player == GomokuEnvironment.BLACK:\n",
    "      for i in (726, 727, 728, 242, 485):\n",
    "        if self.contrib_total[i] > 0:\n",
    "          self.utility = self.player\n",
    "    else:\n",
    "      for i in (0, 1, 2, 243, 486):\n",
    "        if self.contrib_total[i] > 0:\n",
    "          self.utility = self.player\n",
    "\n",
    "    self.player = -self.player\n",
    "\n",
    "  def backward(self):\n",
    "    a = self.path.pop(-1)\n",
    "    i, j = a\n",
    "\n",
    "    self.player = -self.player\n",
    "    \n",
    "    self.action_space.add(a)\n",
    "\n",
    "    self.state[i, j] = GomokuEnvironment.EMPTY\n",
    "    self.__update_contrib_total(i, j, 0, 1, -1)\n",
    "    self.__update_contrib_total(i, j, 1, 0, -1)\n",
    "    self.__update_contrib_total(i, j, 1, 1, -1)\n",
    "    self.__update_contrib_total(i, j, 1, -1, -1)\n",
    "\n",
    "    self.utility = 0\n",
    "\n",
    "    return a\n",
    "\n",
    "  def reset(self):\n",
    "    self.state = torch.zeros((self.n, self.n), dtype=torch.int8).to(device)\n",
    "    self.player = GomokuEnvironment.BLACK\n",
    "    self.utility = 0\n",
    "    self.path = []\n",
    "    \n",
    "    self.__init_action_space()\n",
    "    self.__init_contrib_total()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
      "-820.800048828125 389022.84375 -152931856.0 60886175744.0 -20980581793792.0 6905501031858176.0 -2.2119074702452326e+18 7.305884978820866e+20 -2.1706982710196197e+23 4.7129581822234515e+25 -9.296920533642229e+27 1.5010319163161041e+30 -2.7543430847201385e+32 4.629834840277389e+34 -5.207294115873478e+36 inf nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan \n"
     ]
    }
   ],
   "source": [
    "class GTDLearning(GomokuEnvironment):\n",
    "  def __init__(self, n, gamma):\n",
    "    super().__init__(n)\n",
    "\n",
    "    self.gamma = gamma\n",
    "\n",
    "  def random_policy(self):\n",
    "    return random.choice(list(self.get_action_space()))\n",
    "\n",
    "  def greedy_policy(self):\n",
    "    rv = (-self.get_player() * math.inf, None)\n",
    "    for a in self.get_action_space():\n",
    "      self.forward(a)\n",
    "      rv = max(rv, (self.get_eval(), a))\n",
    "      self.backward()\n",
    "    return rv[1]\n",
    "\n",
    "  def epsilon_greedy_policy(self, epsilon):\n",
    "    if random.random() > epsilon:\n",
    "      return self.greedy_policy()\n",
    "    else:\n",
    "      return self.random_policy()\n",
    "\n",
    "  def training_loop(self, training_episodes=2, eta=0.1, epsilon_low=0.05, epsilon_high=1.0, epsilon_rate=0.0005):\n",
    "    def epsilon(episode):\n",
    "      return epsilon_low + (epsilon_high - epsilon_low) * np.exp(-episode * epsilon_rate)\n",
    "\n",
    "    for episode in range(training_episodes):\n",
    "      self.reset()\n",
    "      while not self.is_end():\n",
    "        a = self.epsilon_greedy_policy(epsilon(episode))\n",
    "        if a == None:\n",
    "          a = self.random_policy()\n",
    "\n",
    "        v_0 = self.get_eval()\n",
    "        grad = self.get_eval_grad()\n",
    "        self.forward(a)\n",
    "        v_f = self.get_eval()\n",
    "        r = self.get_utility()\n",
    "\n",
    "        print(v_0.item(), end=' ')\n",
    "\n",
    "        self.w.add_(eta * ((r + self.gamma * v_f) - v_0) * grad)\n",
    "      print()\n",
    "\n",
    "learner = GTDLearning(9, 0.8)\n",
    "learner.training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan])\n"
     ]
    }
   ],
   "source": [
    "print(learner.w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor([ 5.4962e+07, -9.4583e+07,  5.3028e+08,  3.8073e+08, -9.9378e+07,\n",
    "        -2.4892e+08,  3.7594e+08,  2.2639e+08, -3.9905e+08, -5.4860e+08,\n",
    "         7.6260e+07, -7.3284e+07, -5.5339e+08, -7.0294e+08, -7.8079e+07,\n",
    "        -2.2762e+08,  2.2762e+08,  7.8079e+07,  7.0294e+08,  5.5339e+08,\n",
    "         7.3284e+07, -7.6260e+07,  5.4860e+08,  3.9905e+08, -2.2639e+08,\n",
    "        -3.7594e+08,  2.4892e+08,  9.9378e+07, -3.8073e+08, -5.3028e+08,\n",
    "         9.4583e+07, -5.4962e+07], requires_grad=True)\n",
    "00000 +\n",
    "00001 -\n",
    "00010 +\n",
    "00011 +\n",
    "00100 -\n",
    "00101 -\n",
    "00110 +\n",
    "00111 +\n",
    "01000 -\n",
    "01001 -\n",
    "01010 +\n",
    "01011 -\n",
    "01100 -\n",
    "01101 -\n",
    "01110 -\n",
    "01111 -\n",
    "10000 +\n",
    "10001 +\n",
    "10010 +\n",
    "10011 +\n",
    "10100 +\n",
    "10101 -\n",
    "10110 +\n",
    "10111 +\n",
    "11000 -\n",
    "11001 -\n",
    "11010 +\n",
    "11011 +\n",
    "11100 -\n",
    "11101 -\n",
    "11110 +\n",
    "11111 -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
