{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLDR:\n",
    "- A Markov decision process is defined by:\n",
    "  - The states (and whether or not it's an end state)\n",
    "  - The actions transitioning between those states\n",
    "  - The probability of the transitions given an action and the states\n",
    "  - The reward of the transitions given the action and the states\n",
    "- A policy navigates a Markov decision process\n",
    "- A policy can be trained using value iteration:\n",
    "  - Monte Carlo method:\n",
    "    - Model-based\n",
    "    - Model-free\n",
    "  - Temporal Difference learning:\n",
    "    - SARSA\n",
    "    - Q-Learning\n",
    "    - TD(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Environment ***\n",
      "S F F F F\n",
      "F F F F F\n",
      "F H F F F\n",
      "F F H H F\n",
      "H F F F G\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class FrozenLakeEnvironment:\n",
    "  dirs = [(-1, 0), (0, -1), (1, 0), (0, 1)]\n",
    "\n",
    "  def __init__(self, n, is_slippery=False):\n",
    "    self.n = n\n",
    "    self.is_slippery = is_slippery\n",
    "    self.s = 0\n",
    "\n",
    "    self.__init_map()\n",
    "\n",
    "  def __str__(self):\n",
    "    rv = \"\"\n",
    "    for i in range(self.n):\n",
    "      for j in range(self.n):\n",
    "        s = self.__encode(i, j)\n",
    "        rv += ('S' if s == self.s else self.map[s]) + ' '\n",
    "      rv = rv[:-1] + '\\n'\n",
    "    return rv[:-1]\n",
    "\n",
    "  def __init_map(self):\n",
    "    self.map = ['F'] * (self.n * self.n)\n",
    "    self.map[0] = 'S'\n",
    "    self.map[-1] = 'G'\n",
    "\n",
    "    holes = self.n - 1\n",
    "    for _ in range(holes):\n",
    "      i = 0\n",
    "      while self.map[i] != 'F':\n",
    "        i = random.randrange(0, self.n * self.n)\n",
    "      self.map[i] = 'H'\n",
    "\n",
    "    self.map[0] = 'F'\n",
    "\n",
    "  def __encode(self, i, j):\n",
    "    return i * self.n + j\n",
    "  \n",
    "  def __decode(self, s):\n",
    "    return s // self.n, s % self.n\n",
    "\n",
    "  def can_move(self, s, a):\n",
    "    i, j = self.__decode(s)\n",
    "    di, dj = FrozenLakeEnvironment.dirs[a]\n",
    "\n",
    "    return i + di >= 0 and i + di < self.n and j + dj >= 0 and j + dj < self.n\n",
    "\n",
    "  def get_successor(self, s, a):\n",
    "    if not self.can_move(s, a):\n",
    "      return s\n",
    "    \n",
    "    i, j = self.__decode(s)\n",
    "    di, dj = FrozenLakeEnvironment.dirs[a]\n",
    "\n",
    "    return self.__encode(i + di, j + dj)\n",
    "\n",
    "  def get_observation_space(self):\n",
    "    return list(range(self.n * self.n))\n",
    "\n",
    "  def get_action_space(self, s=None):\n",
    "    if s == None:\n",
    "      return list(range(len(FrozenLakeEnvironment.dirs)))\n",
    "    \n",
    "    rv = []\n",
    "    for i in range(len(FrozenLakeEnvironment.dirs)):\n",
    "      if self.can_move(s, i):\n",
    "        rv.append(i)\n",
    "    return rv\n",
    "\n",
    "  def get_transition_probability(self, s_0, a, s_f):\n",
    "    space = self.get_action_space(s_0)\n",
    "    probs = [0] * len(space)\n",
    "    if self.is_slippery:\n",
    "      for i in range(len(space)):\n",
    "        probs[i] = (1 / 5) * (1 / len(space))\n",
    "      probs[space.index(a)] += 4 / 5\n",
    "    else:\n",
    "      probs[space.index(a)] = 1\n",
    "\n",
    "    rv = 0\n",
    "    for a_t, prob in zip(space, probs):\n",
    "      s_t = self.__get_successor(s_0, a_t)\n",
    "      if s_t == s_f:\n",
    "        rv += prob\n",
    "\n",
    "    return rv\n",
    "\n",
    "  def get_reward(self, s_0, a, s_f):\n",
    "    if self.map[s_f] == 'H':\n",
    "      return -1\n",
    "    if self.map[s_f] == 'F':\n",
    "      return 0\n",
    "    if self.map[s_f] == 'G':\n",
    "      return 1\n",
    "    \n",
    "  def is_end(self, s):\n",
    "    return self.map[s] == 'H' or self.map[s] == 'G'\n",
    "  \n",
    "  def observe(self):\n",
    "    return self.s\n",
    "  \n",
    "  def act(self, a):\n",
    "    if self.is_slippery:\n",
    "      if random.randint(1, 5) == 1:\n",
    "        a = random.choice(self.get_action_space())\n",
    "    \n",
    "    self.s = self.get_successor(self.s, a)\n",
    "  \n",
    "  def reset(self):\n",
    "    self.s = 0\n",
    "\n",
    "env = FrozenLakeEnvironment(5, True)\n",
    "print(\"*** Environment ***\")\n",
    "print(env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of a Markov decision process:\n",
    "1. A starting state `s_start`\n",
    "2. A space of all possible actions `actions(s)` from state `s`\n",
    "3. Transition probabilities `T(s, a, s')` representing the probability of reaching `s'` from `s` after just one action `a`\n",
    "4. Rewards `Reward(s, a, s')` representing the reward of transitioning between `s` to `s'` via action `a`\n",
    "5. `IsEnd(s)` to determine if the state `s` is an end state\n",
    "6. A discount factor `gamma` to discount rewards from states further away\n",
    "\n",
    "Markov decision process vocabulary:\n",
    "- Policy: a function that chooses an action for any given state\n",
    "  - `pi(s) = a`\n",
    "- Utility of a path: the discounted sum of rewards on a path\n",
    "  - `u(s0, s1, ..., sk) = r1 * gamma ** 0 + r2 * gamma ** 1 + ... + rk * gamma ** (k - 1)`\n",
    "- Q-value: the expected utility of all paths starting from `s` after taking action `a` following policy `pi`\n",
    "  - `Qpi(s, a) = sum(T(s, a, s') * (Reward(s, a, s') + gamma * Qpi(s, pi(s)))) for all s'`\n",
    "- Value: the expected utility of all paths starting from `s` following policy `pi`\n",
    "  - `V(s) = sum(T(s, a, s') * (Reward(s, a, s') + gamma * V(s))) for all s'`\n",
    "\n",
    "Markov decision process training vocabulary:\n",
    "- Optimal Q-value: the maximum Q-value attained by the policy\n",
    "- Optimal value: the maximum value attained by the policy\n",
    "- Optimal policy: the policy that achieves the optimal values\n",
    "- Initialization: all values are initialized to a null value\n",
    "- Iteration: values are updated per iteration:\n",
    "  - Updates are defined by the training method\n",
    "  - The goal is to reach the optimal Q-value/value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-free Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Model-free Monte Carlo ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 7614.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Policy ***\n",
      "R R R R D\n",
      "U U R R D\n",
      "U U U R D\n",
      "U D U U D\n",
      "U R R R U\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 11109.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Evaluation ***\n",
      "0.5387692817714264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class ModelFreeMonteCarlo:\n",
    "  def __init__(self, env, gamma=0.95):\n",
    "    self.env = env\n",
    "    self.gamma = gamma\n",
    "\n",
    "    action_len = len(env.get_action_space())\n",
    "    observation_len = len(env.get_observation_space())\n",
    "\n",
    "    self.q_table = [[-1 for i in range(action_len)] for j in range(observation_len)]\n",
    "    self.updates = [[0 for i in range(action_len)] for j in range(observation_len)]\n",
    "\n",
    "  def __str__(self):\n",
    "    rv = \"\"\n",
    "    for i in range(self.env.n):\n",
    "      for j in range(self.env.n):\n",
    "        rv += ['U', 'L', 'D', 'R'][self.greedy_policy(i * self.env.n + j)] + ' '\n",
    "      rv = rv[:-1] + '\\n'\n",
    "    return rv[:-1]\n",
    "\n",
    "  def random_policy(self, s):\n",
    "    return random.choice(self.env.get_action_space(s))\n",
    "\n",
    "  def greedy_policy(self, s):\n",
    "    return np.argmax(self.q_table[s])\n",
    "\n",
    "  def epsilon_greedy_policy(self, s, epsilon):\n",
    "    if random.random() > epsilon:\n",
    "      return self.greedy_policy(s)\n",
    "    else:\n",
    "      return self.random_policy(s)\n",
    "\n",
    "  def training_loop(self, training_episodes=10000, epsilon_low=0.05, epsilon_high=1.0, epsilon_rate=0.0005):\n",
    "    def epsilon(episode):\n",
    "      return epsilon_low + (epsilon_high - epsilon_low) * np.exp(-episode * epsilon_rate)\n",
    "    \n",
    "    for episode in tqdm(range(training_episodes)):\n",
    "      sar_arr = []\n",
    "      self.env.reset()\n",
    "      while not self.env.is_end(self.env.observe()):\n",
    "        s_0 = self.env.observe()\n",
    "        a = self.epsilon_greedy_policy(s_0, epsilon(episode))\n",
    "        if not self.env.can_move(s_0, a):\n",
    "          a = self.random_policy(s_0)\n",
    "\n",
    "        self.env.act(a)\n",
    "        s_f = self.env.observe()\n",
    "        sar_arr.append((s_0, a, self.env.get_reward(s_0, a, s_f)))\n",
    "      \n",
    "      sau_arr = []\n",
    "      u = 0\n",
    "      for s, a, r in sar_arr[::-1]:\n",
    "        u = u * self.gamma + r\n",
    "        sau_arr.append((s, a, u))\n",
    "\n",
    "      visited = set()\n",
    "      for s, a, u in sau_arr[::-1]:\n",
    "        if not (s, a) in visited:\n",
    "          visited.add((s, a))\n",
    "\n",
    "          eta = 1 / (1 + self.updates[s][a])\n",
    "          self.q_table[s][a] = (1 - eta) * self.q_table[s][a] + eta * u\n",
    "          self.updates[s][a] += 1\n",
    "  \n",
    "  def evaluation_loop(self, evaluation_episodes=100, gamma=0.95):\n",
    "    u_sum = 0\n",
    "    for episode in tqdm(range(evaluation_episodes)):\n",
    "      r_arr = []\n",
    "      self.env.reset()\n",
    "      while not self.env.is_end(self.env.observe()):\n",
    "        s_0 = self.env.observe()\n",
    "        a = self.greedy_policy(s_0)\n",
    "        if not self.env.can_move(s_0, a):\n",
    "          a = self.random_policy(s_0)\n",
    "\n",
    "        self.env.act(a)\n",
    "        s_f = self.env.observe()\n",
    "        r_arr.append(self.env.get_reward(s_0, a, s_f))\n",
    "      \n",
    "      u = 0\n",
    "      for r in r_arr[::-1]:\n",
    "        u = u * gamma + r\n",
    "      u_sum += u\n",
    "    \n",
    "    return u_sum / evaluation_episodes\n",
    "\n",
    "learner = ModelFreeMonteCarlo(env)\n",
    "print(\"*** Model-free Monte Carlo ***\")\n",
    "print()\n",
    "\n",
    "learner.training_loop()\n",
    "print(\"*** Policy ***\")\n",
    "print(learner)\n",
    "print()\n",
    "\n",
    "evaluation = learner.evaluation_loop()\n",
    "print(\"*** Evaluation ***\")\n",
    "print(evaluation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-free Monte Carlo characteristics:\n",
    "- Estimator of Q-values\n",
    "  - Compared to *model-based* Monte Carlo, which estimates transition probability and reward, which are then used to estimate the Q-values\n",
    "- Finds the \"average\" Q-value (utility `u`) for any given `s` and `a`\n",
    "  - `Q(s, a) = (1 - eta) * Q(s, a) + eta * u`\n",
    "  - where `eta = 1 / (1 + # of samples for (s, a))`\n",
    "\n",
    "About the epsilon-greedy policy:\n",
    "- The epsilon-greedy policy balances exploration and exploitation\n",
    "  - Exploration searches for new behavior\n",
    "  - Exploitation follows the optimal behavior\n",
    "- Can be used in most, if not all, training methods (used in all training methods covered in this notebook)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SARSA ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:10<00:00, 924.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Policy ***\n",
      "R R D L L\n",
      "R R L U U\n",
      "U U U R U\n",
      "U L U U U\n",
      "U D L L U\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2380.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Evaluation ***\n",
      "-0.2577885476058239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class SARSA:\n",
    "  def __init__(self, env, gamma=0.95):\n",
    "    self.env = env\n",
    "    self.gamma = gamma\n",
    "\n",
    "    action_len = len(env.get_action_space())\n",
    "    observation_len = len(env.get_observation_space())\n",
    "\n",
    "    self.q_table = [[-1 for i in range(action_len)] for j in range(observation_len)]\n",
    "\n",
    "  def __str__(self):\n",
    "    rv = \"\"\n",
    "    for i in range(self.env.n):\n",
    "      for j in range(self.env.n):\n",
    "        rv += ['U', 'L', 'D', 'R'][self.greedy_policy(i * self.env.n + j)] + ' '\n",
    "      rv = rv[:-1] + '\\n'\n",
    "    return rv[:-1]\n",
    "\n",
    "  def random_policy(self, s):\n",
    "    return random.choice(self.env.get_action_space(s))\n",
    "\n",
    "  def greedy_policy(self, s):\n",
    "    return np.argmax(self.q_table[s])\n",
    "\n",
    "  def epsilon_greedy_policy(self, s, epsilon):\n",
    "    if random.random() > epsilon:\n",
    "      return self.greedy_policy(s)\n",
    "    else:\n",
    "      return self.random_policy(s)\n",
    "\n",
    "  def training_loop(self, training_episodes=10000, eta=0.7, epsilon_low=0.05, epsilon_high=1.0, epsilon_rate=0.0005):\n",
    "    def epsilon(episode):\n",
    "      return epsilon_low + (epsilon_high - epsilon_low) * np.exp(-episode * epsilon_rate)\n",
    "\n",
    "    for episode in tqdm(range(training_episodes)):\n",
    "      self.env.reset()\n",
    "      while not self.env.is_end(self.env.observe()):\n",
    "        s_0 = self.env.observe()\n",
    "        a_0 = self.epsilon_greedy_policy(s_0, epsilon(episode))\n",
    "        if not self.env.can_move(s_0, a_0):\n",
    "          a_0 = self.random_policy(s_0)\n",
    "\n",
    "        self.env.act(a_0)\n",
    "        s_f = self.env.observe()\n",
    "        a_f = self.greedy_policy(s_f)\n",
    "        r = self.env.get_reward(s_0, a_0, s_f)\n",
    "\n",
    "        self.q_table[s_0][a_0] = (1 - eta) * self.q_table[s_0][a_0] + eta * (r + self.gamma * self.q_table[s_f][a_f])\n",
    "  \n",
    "  def evaluation_loop(self, evaluation_episodes=100, gamma=0.95):\n",
    "    u_sum = 0\n",
    "    for episode in tqdm(range(evaluation_episodes)):\n",
    "      r_arr = []\n",
    "      self.env.reset()\n",
    "      while not self.env.is_end(self.env.observe()):\n",
    "        s_0 = self.env.observe()\n",
    "        a = self.greedy_policy(s_0)\n",
    "        if not self.env.can_move(s_0, a):\n",
    "          a = self.random_policy(s_0)\n",
    "\n",
    "        self.env.act(a)\n",
    "        s_f = self.env.observe()\n",
    "        r_arr.append(self.env.get_reward(s_0, a, s_f))\n",
    "      \n",
    "      u = 0\n",
    "      for r in r_arr[::-1]:\n",
    "        u = u * gamma + r\n",
    "      u_sum += u\n",
    "    \n",
    "    return u_sum / evaluation_episodes\n",
    "\n",
    "learner = SARSA(env)\n",
    "print(\"*** SARSA ***\")\n",
    "print()\n",
    "\n",
    "learner.training_loop()\n",
    "print(\"*** Policy ***\")\n",
    "print(learner)\n",
    "print()\n",
    "\n",
    "evaluation = learner.evaluation_loop()\n",
    "print(\"*** Evaluation ***\")\n",
    "print(evaluation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA characteristics:\n",
    "- Estimator of Q-values\n",
    "- Explicitly chooses an `a'` using a policy (hence, \"on-policy\")\n",
    "  - `Q(s, a) = (1 - eta) * Q(s, a) + eta * (r + gamma * Q(s', a'))`\n",
    "  - Similar to branching only to `a'`, as chosen by the policy: `s`, `a` -> `s'` -> `a'`\n",
    "- With respect to the \"cliff-walking\" metaphor, SARSA takes the longer, safer path avoiding the cliff, as its Q-values are dependent on the actual actions chosen by the policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Q-learning ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:21<00:00, 458.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Policy ***\n",
      "D L L L L\n",
      "D L U U U\n",
      "D U D U L\n",
      "R L U U U\n",
      "U R R R U\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 8326.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Evaluation ***\n",
      "-0.3837843224088213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class QLearning:\n",
    "  def __init__(self, env, gamma=0.95):\n",
    "    self.env = env\n",
    "    self.gamma = gamma\n",
    "\n",
    "    action_len = len(env.get_action_space())\n",
    "    observation_len = len(env.get_observation_space())\n",
    "\n",
    "    self.q_table = [[-1 for i in range(action_len)] for j in range(observation_len)]\n",
    "  \n",
    "  def __str__(self):\n",
    "    rv = \"\"\n",
    "    for i in range(self.env.n):\n",
    "      for j in range(self.env.n):\n",
    "        rv += ['U', 'L', 'D', 'R'][self.greedy_policy(i * self.env.n + j)] + ' '\n",
    "      rv = rv[:-1] + '\\n'\n",
    "    return rv[:-1]\n",
    "\n",
    "  def random_policy(self, s):\n",
    "    return random.choice(self.env.get_action_space(s))\n",
    "\n",
    "  def greedy_policy(self, s):\n",
    "    return np.argmax(self.q_table[s])\n",
    "\n",
    "  def epsilon_greedy_policy(self, s, epsilon):\n",
    "    if random.random() > epsilon:\n",
    "      return self.greedy_policy(s)\n",
    "    else:\n",
    "      return self.random_policy(s)\n",
    "\n",
    "  def training_loop(self, training_episodes=10000, eta=0.7, epsilon_low=0.05, epsilon_high=1.0, epsilon_rate=0.0005):\n",
    "    def epsilon(episode):\n",
    "      return epsilon_low + (epsilon_high - epsilon_low) * np.exp(-episode * epsilon_rate)\n",
    "    \n",
    "    for episode in tqdm(range(training_episodes)):\n",
    "      self.env.reset()\n",
    "      while not self.env.is_end(self.env.observe()):\n",
    "        s_0 = self.env.observe()\n",
    "        a_0 = self.epsilon_greedy_policy(s_0, epsilon(episode))\n",
    "        if not self.env.can_move(s_0, a_0):\n",
    "          a_0 = self.random_policy(s_0)\n",
    "\n",
    "        self.env.act(a_0)\n",
    "        s_f = self.env.observe()\n",
    "        r = self.env.get_reward(s_0, a_0, s_f)\n",
    "\n",
    "        self.q_table[s_0][a_0] = (1 - eta) * self.q_table[s_0][a_0] + eta * (r + self.gamma * np.argmax(self.q_table[s_f]))\n",
    "  \n",
    "  def evaluation_loop(self, evaluation_episodes=100, gamma=0.95):\n",
    "    u_sum = 0\n",
    "    for episode in tqdm(range(evaluation_episodes)):\n",
    "      r_arr = []\n",
    "      self.env.reset()\n",
    "      while not self.env.is_end(self.env.observe()):\n",
    "        s_0 = self.env.observe()\n",
    "        a = self.greedy_policy(s_0)\n",
    "        if not self.env.can_move(s_0, a):\n",
    "          a = self.random_policy(s_0)\n",
    "\n",
    "        self.env.act(a)\n",
    "        s_f = self.env.observe()\n",
    "        r_arr.append(self.env.get_reward(s_0, a, s_f))\n",
    "      \n",
    "      u = 0\n",
    "      for r in r_arr[::-1]:\n",
    "        u = u * gamma + r\n",
    "      u_sum += u\n",
    "    \n",
    "    return u_sum / evaluation_episodes\n",
    "\n",
    "learner = QLearning(env)\n",
    "print(\"*** Q-learning ***\")\n",
    "print()\n",
    "\n",
    "learner.training_loop()\n",
    "print(\"*** Policy ***\")\n",
    "print(learner)\n",
    "print()\n",
    "\n",
    "evaluation = learner.evaluation_loop()\n",
    "print(\"*** Evaluation ***\")\n",
    "print(evaluation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning characteristics:\n",
    "- Estimator of Q-values\n",
    "- Does not choose an `a'` using any policy (hence, \"off-policy\")\n",
    "  - Simply takes the maximum of all successors' Q-values\n",
    "  - `Q(s, a) = (1 - eta) * Q(s, a) + eta * (r + gamma * max(Q(s', {a'1, a'2, ... a'n})))`\n",
    "  - Similar to branching to all `a'` and choosing the best one: `s`, `a` -> `s'` -> `a'1`, `a'2`, `...`, `a'n`\n",
    "- With respect to the \"cliff-walking\" metaphor, Q-learning takes the most opportunistic path, since its Q-values are based on the best case scenario from each state and thus, has a higher risk of walking off the cliff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
